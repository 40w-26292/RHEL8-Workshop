:sectnums:
:sectnumlevels: 2
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

:toc:
:toclevels: 1

= Container Fundamentals

In this unit, we will get familiar with Containers and the podman/buildah CLI.  

[discrete]
== Additional Reference Materials

kabbott NOTE: Should update here...may want to highlight the Universal Base Image work that McCarty has done as well....

NOTE: You are not required to reference any additional resources for these exercises.  This is informational only.

    * link:https://developers.redhat.com/blog/2018/02/22/container-terminology-practical-introduction/[A Practical Introduction to Container Terminology - Scott McCarty]


== Connect to Host

There is a dedicated VM we will use for the container exercises.  From workstation.example.com, you should be able to ssh to node3.example.com as 'root' without any prompts for credentials.

.[root@workstation ~]#
----
ssh node3.example.com
----

Now you are ready to begin your exercises with containers.

== Container Basics

=== Container Info

Now have a look at the general docker information.

.[root@node3 ~]#
----
podman info
----

.Your output should look like this
[source,indent=4]
----
host:
  BuildahVersion: 1.6-dev
  Conmon:
    package: podman-1.0.0-2.git921f98f.module+el8+2785+ff8a053f.x86_64
    path: /usr/libexec/podman/conmon
    version: 'conmon version 1.14.0-dev, commit: be8255a19cda8a598d76dfa49e16e337769d4528-dirty'
  Distribution:
    distribution: '"rhel"'
    version: "8.0"
  MemFree: 3282280448
  MemTotal: 3964538880
  OCIRuntime:
    package: runc-1.0.0-54.rc5.dev.git2abd837.module+el8+2769+577ad176.x86_64
    path: /usr/bin/runc
    version: 'runc version spec: 1.0.0'
  SwapFree: 1073737728
  SwapTotal: 1073737728
  arch: amd64
  cpus: 2
  hostname: node3.example.com
  kernel: 4.18.0-67.el8.x86_64
  os: linux
  rootless: false
  uptime: 46m 50.43s
insecure registries:
  registries: []
registries:
  registries:
  - registry.redhat.io
  - quay.io
  - docker.io
store:
  ConfigFile: /etc/containers/storage.conf
  ContainerStore:
    number: 0
  GraphDriverName: overlay
  GraphOptions: null
  GraphRoot: /var/lib/containers/storage
  GraphStatus:
    Backing Filesystem: xfs
    Native Overlay Diff: "true"
    Supports d_type: "true"
  ImageStore:
    number: 0
  RunRoot: /var/run/containers/storage
----

=== Container Image Management

==== Current List of Images

Now have a look at the general container information.

.[root@node3 ~]#
----
podman images
----

.Your output should look like this
[source,indent=4]
----
registry.redhat.io/rhel7   latest   5044f6040ea5   2 weeks ago   214 MB
----

==== How to Pull Images

.[root@docker ~]#
----
podman pull rhel7.5 --creds 'username:password'
----

.[root@docker ~]#
----
podman pull rhel6 --creds 'username:password'
----

Container images can also be tagged with convenient (ie:custom names).  This could make it more intuitive to understand what they contain, esspecialy after an image has been customized.

Later you will create a custom image based on an official Red Hat Enterprise Linux container image.

NOTE: The link:https://access.redhat.com/containers[Red Hat Container Catalog] (RHCC) provides a convenient service to locate certified container images built and supported by Red Hat.  You can also view the "security evaluation" for each image.

==== How to Remove Images

.[root@node3 ~]#
----
podman images
----

.[root@node3 ~]#
----
podman rmi rhel6
----

=== Essential Container Commands

*podman images* - list images

*podman ps* - lists running containers

*podman pull* - pulls (copies) container image from repository (ie: redhat and/or docker hub)

*podman run* - run a docker container

*podman logs* - display logs of a container (can be used with --follow)

*podman rm* - remove one or more containers

*podman rmi* - remove one or more images

*podman stop* - stops one or more containers

*podman kill $(podman ps -q)* - kill all running containers

*podman rm $(podman ps -a -q)* - deletes all stopped containers

== Your First Container

=== Hello World

.[root@node3 ~]#
----
podman run rhel7 echo "hello world"
----

.Your output should look like this
[source,indent=4]
----
hello world
----

Well that was really boring!! What did we learn from this?  For starters, you should have noticed how fast the container launched and then concluded.  Compare that with traditinal virtualization where:
    * you power up, 
    * wait for bios, 
    * wait for grub, 
    * wait for the kernel to boot and initialize resources,
    * pivot root, 
    * launch all the services, and then finally
    * run the application

Let us run a few more commands to see what else we can gleen.

.[root@node3 ~]#
----
podman ps -a
----

.Your output should look like this
[source,indent=4]
----
CONTAINER ID  IMAGE                            COMMAND           CREATED         STATUS                     PORTS  NAMES
ce7b462b58af  registry.redhat.io/rhel7:latest  echo hello world  33 seconds ago  Exited (0) 31 seconds ago         competent_leavitt
----

Now let us run the exact same command again.

.[root@node3 ~]#
----
podman run rhel7 echo "hello world"
----

.Your output should look like this
[source,indent=4]
----
hello world
----

Check out 'podman info' one more time and you should notice a few changes.

.[root@node3 ~]#
----
podman info
----

.Your output should look like this
[source,indent=4]
----
host:
  BuildahVersion: 1.6-dev
  Conmon:
    package: podman-1.0.0-2.git921f98f.module+el8+2785+ff8a053f.x86_64
    path: /usr/libexec/podman/conmon
    version: 'conmon version 1.14.0-dev, commit: be8255a19cda8a598d76dfa49e16e337769d4528-dirty'
  Distribution:
    distribution: '"rhel"'
    version: "8.0"
  MemFree: 2743906304
  MemTotal: 3964538880
  OCIRuntime:
    package: runc-1.0.0-54.rc5.dev.git2abd837.module+el8+2769+577ad176.x86_64
    path: /usr/bin/runc
    version: 'runc version spec: 1.0.0'
  SwapFree: 1073737728
  SwapTotal: 1073737728
  arch: amd64
  cpus: 2
  hostname: node3.example.com
  kernel: 4.18.0-67.el8.x86_64
  os: linux
  rootless: false
  uptime: 55m 13.64s
insecure registries:
  registries: []
registries:
  registries:
  - registry.redhat.io
  - quay.io
  - docker.io
store:
  ConfigFile: /etc/containers/storage.conf
  ContainerStore:
    number: 2
  GraphDriverName: overlay
  GraphOptions: null
  GraphRoot: /var/lib/containers/storage
  GraphStatus:
    Backing Filesystem: xfs
    Native Overlay Diff: "true"
    Supports d_type: "true"
  ImageStore:
    number: 2
  RunRoot: /var/run/containers/storage
----

You should notice that the number of containers (ContainerStore) has incremented to 2, and that the number of ImageStore(s) has grown.  

=== Cleanup

Run 'podman ps -a' to the IDs of the exited containers.

.[root@node3 ~]#
----
podman ps -a
----

.Your output should look like this
[source,indent=4]
----
CONTAINER ID  IMAGE                            COMMAND           CREATED        STATUS                    PORTS  NAMES
ccf81a8232e9  registry.redhat.io/rhel7:latest  echo hello world  2 minutes ago  Exited (0) 2 minutes ago         epic_golick
ce7b462b58af  registry.redhat.io/rhel7:latest  echo hello world  3 minutes ago  Exited (0) 3 minutes ago         competent_leavitt
----

Using the container UIDs from the above output, you can now clean up the 'exited' containers.

.[root@node3 ~]#
----
podman rm <CONTAINER-ID> <CONTAINER-ID>
----

Now you should be able to run 'podman ps -a' again, and the results should come back empty.

.[root@node3 ~]#
----
podman ps -a
----

== Exploring Container Namespaces

=== UTS / Hostname

.[root@node3 ~]#
----
podman run rhel7 cat /proc/sys/kernel/hostname
----

.Your output should look like this
[source,indent=4]
----
a28142f3eae0
----

So what we have learned here is that the hostname in the container's namespace is NOT the same as the host platform (node3.example.com).  It is unique and is by default identical to the container's ID.  You can verify this with 'podman ps -a'.

.[root@node3 ~]#
----
podman ps -a
----

.Your output should look like this
[source,indent=4]
----
CONTAINER ID  IMAGE                            COMMAND               CREATED         STATUS                     PORTS  NAMES
a28142f3eae0  registry.redhat.io/rhel7:latest  cat /proc/sys/ker...  48 seconds ago  Exited (0) 47 seconds ago         romantic_swanson
----


=== Process ID

.[root@node3 ~]#
----
podman run rhel7 ps -ef
----

.Your output should look like this
[source,indent=4]
----
UID        PID  PPID  C STIME TTY          TIME CMD
root         1     0  0 19:53 ?        00:00:00 ps -ef
----

=== Network

Now let us run a command to report the network configuration from within the a container's namespace.

.[root@node3 ~]#
----
podman run rhel7 ip addr show eth0
----

.Your output should look like this
[source,indent=4]
----
container create failed: container_linux.go:336: starting container process caused "exec: \"ip\": executable file not found in $PATH"
: internal libpod error
----

What just happened?

For the most part, containers are not meant for interactive (user) sessions.  In this instance, then image that we are using (ie: rhel7) does not have the traditional commmandline utilities a user might expect.  Common tools to configure network interfaces like 'ip' simply aren't there.

So for this exercise, we leverage something called a 'bind mount' to effectively mirror a portion of the host's filesystem into the container's namespace.  Bind mounts are declared using the '-v' option.  In the example below, /usr/sbin from the host will be exposed and accessible to the containers namespace mounted at '/usr/bin' (ie: /usr/sbin:/usr/sbin).

NOTE: Using bind mounts is generally suitable for debugging, but not a good practice as a design decision for enterprise container strategies.  After all, creating dependencies between applications and host operating systems is what we are trying to get away from.

.[root@noede3 ~]#
----
podman run -v /usr/sbin:/usr/sbin -v /usr/lib64:/usr/lib64 --rm rhel7 /usr/sbin/ip addr show eth0
----

.Your output should look like this
[source,indent=4]
----
3: eth0@if10: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default
    link/ether 8a:ce:7f:ea:c7:9a brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.88.0.8/16 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::88ce:7fff:feea:c79a/64 scope link tentative
       valid_lft forever preferred_lft forever
----

A couple more commands to understand the network setup.

Let us begin by examining the '/etc/hosts' file.

.[root@node3 ~]#
----
podman run --rm rhel7 cat /etc/hosts
----

.Your output should look like this
[source,indent=4]
----
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
10.88.0.9       aa2204f3cd29
----

How does the container resolve hostnames (ie: DNS)?

.[root@node3 ~]#
----
podman run --rm rhel7 cat /etc/resolv.conf
----

.Your output should look like this
[source,indent=4]
----
search example.com
nameserver 10.0.0.2
----

Take a look at the routing table.
Pay attention now, the route command is in '/usr/bin'.  Take a look at the routing table for the container namespace.

.[root@node3 ~]#
----
podman run -v /usr/sbin:/usr/sbin --rm rhel7 route -n
----

.Your output should look like this
[source,indent=4]
----
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         10.88.0.1       0.0.0.0         UG    0      0        0 eth0
10.88.0.0       0.0.0.0         255.255.0.0     U     0      0        0 eth0
----


=== Filesystem

.[root@node3 ~]#
----
podman run rhel7 df -h
----

.Your output should look like this
[source,indent=4]
----
Filesystem      Size  Used Avail Use% Mounted on
overlay         8.0G  1.9G  6.2G  24% /
tmpfs            64M     0   64M   0% /dev
tmpfs           1.9G  8.6M  1.9G   1% /etc/hosts
shm              63M     0   63M   0% /dev/shm
tmpfs           1.9G     0  1.9G   0% /sys/fs/cgroup
tmpfs           1.9G     0  1.9G   0% /proc/acpi
tmpfs           1.9G     0  1.9G   0% /proc/scsi
tmpfs           1.9G     0  1.9G   0% /sys/firmware
----

You were introduced to Bind-Mounts in the previous section.  Let us examine what the filesystems looks like with an active Bind-Mount.

.[root@node3 ~]#
----
podman run -v /usr/bin:/usr/bin rhel7 df -h
----

.Your output should look like this
[source,indent=4]
----
Filesystem             Size  Used Avail Use% Mounted on
overlay                8.0G  1.9G  6.2G  24% /
tmpfs                   64M     0   64M   0% /dev
tmpfs                  1.9G  8.6M  1.9G   1% /etc/hosts
/dev/mapper/rhel-root  8.0G  1.9G  6.2G  24% /usr/bin
shm                     63M     0   63M   0% /dev/shm
tmpfs                  1.9G     0  1.9G   0% /sys/fs/cgroup
tmpfs                  1.9G     0  1.9G   0% /proc/acpi
tmpfs                  1.9G     0  1.9G   0% /proc/scsi
tmpfs                  1.9G     0  1.9G   0% /sys/firmware
----

Notice above how there is now a dedicated mount point for /usr/bin.  Bind-Mounts can be a very powerful tool (primarily for diagnostics) to termporarily inject tools and files that are not normally part of a container image.  Using bind mounts as a design decision for enterprise container strategies is folly.  Creating direct dependencies between containerized applications and host operating systems is what we are trying to get away from.


Let us clean up your environment before proceeding

.[root@node3 ~]#
----
podman kill $(podman ps -q)

podman rm $(podman ps -a -q)
----

=== User -- Is this whole section even relevant in podman/buildah world?

By default, the docker daemon runs applications with root privileges.  As you will see later, this creates an enormous security risk.  You can however configure docker to enable a user namespaces mapping option which allows you to run applications with root privilege inside a container, but have them run as a different non-privileged user on the host.  At this time only a single UID and GID can be mapped per daemon.

User Namespaces and container security are covered more thoroughly in the Openshift parts of this workshop.

NOTE: For more information on configuring the docker daemon with User Namespaces please see link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_atomic_host/7/html-single/getting_started_with_containers/index[Getting Started With Containers]

== Your First Containerized Application

=== Setup -- this will need a rewrite....should really use our httpd container for this.

.[root@node3 ~]#
----
mkdir -p /var/www/html

echo "Server up and running" > /var/www/html/test.txt

restorecon -Rv /var/www
----

=== Deployment

.[root@master ~]#
----
cheat-docker-webserver.sh
----

[NOTE]
====
_Native command(s) to deploy SimpleHTTPServer_
----
docker run -d --name="python_web" \
    -v /usr/bin:/usr/bin \
    -v /usr/lib64:/usr/lib64 \
    -v /var/www/html:/var/www/html \
    -w /var/www/html  \
    -p 8000:8000 \
    rhel7 /bin/python -m SimpleHTTPServer 8000
----
====

=== Validation

.[root@docker ~]#
----
pgrep -laf SimpleHTTP
----

.Your output should look like this
[source,indent=4]
----
423 /bin/python -m SimpleHTTPServer 8000
----

On the host, we see a python process running using module SimpleHTTPServie on port 8000.  That's good!

Now let's introduce a commandline utility 'lsns' to check out the namespaces.

.[root@docker ~]#
----
lsns
----

.Your output should look like this
[source,indent=4]
----
        NS TYPE  NPROCS   PID USER   COMMAND
4026531836 pid      126     1 root   /usr/lib/systemd/systemd --switched-root --system --deserialize 22
4026531837 user     127     1 root   /usr/lib/systemd/systemd --switched-root --system --deserialize 22
4026531838 uts      126     1 root   /usr/lib/systemd/systemd --switched-root --system --deserialize 22
4026531839 ipc      126     1 root   /usr/lib/systemd/systemd --switched-root --system --deserialize 22
4026531840 mnt      122     1 root   /usr/lib/systemd/systemd --switched-root --system --deserialize 22
4026531856 mnt        1    13 root   kdevtmpfs
4026531956 net      126     1 root   /usr/lib/systemd/systemd --switched-root --system --deserialize 22
4026532143 mnt        1   491 root   /usr/lib/systemd/systemd-udevd
4026532162 mnt        1   611 chrony /usr/sbin/chronyd
4026532163 mnt        1   635 root   /usr/sbin/NetworkManager --no-daemon
4026532170 mnt        1   423 root   /bin/python -m SimpleHTTPServer 8000
4026532171 uts        1   423 root   /bin/python -m SimpleHTTPServer 8000
4026532172 ipc        1   423 root   /bin/python -m SimpleHTTPServer 8000
4026532173 pid        1   423 root   /bin/python -m SimpleHTTPServer 8000
4026532175 net        1   423 root   /bin/python -m SimpleHTTPServer 8000
----

Again on the host, we see a python process running using the mnt uts ipc pid and net namespaces.  That's good too!

Well since we explored namespaces, we may as well have a look and discuss the control-groups aligned with our process.

.[root@docker ~]#
----
systemd-cgls 
----

.Your output should look like this
[source,indent=4]
----
├─1 /usr/lib/systemd/systemd --switched-root --system --deserialize 22
├─user.slice
│ └─user-0.slice
│   └─session-11.scope
│     ├─ 728 systemd-cgls
│     ├─ 729 systemd-cgls
│     ├─2941 sshd: root@pts/0
│     └─2944 -bash
└─system.slice
  ├─docker-d8319a624c80ff5b8ce5afc049efc636fc500954b1e8383076feebfcb4549279.scope
  │ └─423 /bin/python -m SimpleHTTPServer 8000
  ├─docker.service
  │ ├─ 406 /usr/libexec/docker/docker-proxy-current -proto tcp -host-ip 0.0.0.0 -host-port 8000 -container-ip 172.17.0.2 -container-port 8000
  │ ├─ 410 /usr/bin/docker-containerd-shim-current d8319a624c80ff5b8ce5afc049efc636fc500954b1e8383076feebfcb4549279 /var/run/docker/libcontainerd/d8319a624c80ff5b8ce5afc049efc636fc500954b1e8383076feebfcb4549279 /
  │ ├─2860 /usr/bin/dockerd-current --add-runtime docker-runc=/usr/libexec/docker/docker-runc-current --default-runtime=docker-runc --authorization-plugin=rhel-push-plugin --exec-opt native.cgroupdriver=systemd -
  │ └─2864 /usr/bin/docker-containerd-current -l unix:///var/run/docker/libcontainerd/docker-containerd.sock --metrics-interval=0 --start-timeout 2m --state-dir /var/run/docker/libcontainerd/containerd --shim doc
  ├─docker-distribution.service
  │ └─2323 /usr/bin/registry serve /etc/docker-distribution/registry/config.yml
  ├─rhel-push-plugin.service
  │ └─2229 /usr/libexec/docker/rhel-push-plugin
  ├─dm-event.service
  │ └─2075 /usr/sbin/dmeventd -f
  ├─rsyslog.service
  │ └─906 /usr/sbin/rsyslogd -n
  <... SNIP ...>
----


.[root@docker ~]#
----
netstat -tulpn | grep 8000
----

.Your output should look like this
[source,indent=4]
----
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      903/sshd
tcp        0      0 127.0.0.1:25            0.0.0.0:*               LISTEN      1208/master
tcp6       0      0 :::8000                 :::*                    LISTEN      406/docker-proxy-cu
tcp6       0      0 :::5000                 :::*                    LISTEN      2323/registry
tcp6       0      0 :::22                   :::*                    LISTEN      903/sshd
tcp6       0      0 ::1:25                  :::*                    LISTEN      1208/master
udp        0      0 0.0.0.0:5353            0.0.0.0:*                           602/avahi-daemon: r
udp        0      0 127.0.0.1:323           0.0.0.0:*                           611/chronyd
udp        0      0 0.0.0.0:58000           0.0.0.0:*                           602/avahi-daemon: r
udp6       0      0 ::1:323                 :::*                                611/chronyd
----

Just pointing out that that there is now a service hanging on port 8000 proxying the network traffic to the container.

Now let us see if the simple web server is working.

.[root@docker ~]#
----
curl localhost:8000/test.txt
----

.Your output should look like this
[source,indent=4]
----
Server up and running
----

=== Cleanup

.[root@docker ~]#
----
docker stop python_web

docker rm python_web
----



== Exploring Container Security

Now it is time to examine security.  Start be re-launching the container from our last exercise.

=== Standard (lack-of) Security

.[root@master ~]#
----
cheat-docker-webserver.sh
----

[NOTE]
====
_Native command(s) to deploy SimpleHTTPServer_
----
docker run -d --name="python_web" \
    -v /usr/bin:/usr/bin \
    -v /usr/lib64:/usr/lib64 \
    -v /var/www/html:/var/www/html \
    -w /var/www/html  \
    -p 8000:8000 \
    rhel7 /bin/python -m SimpleHTTPServer 8000
----
====

Now you will start a shell that inherits the namespaces from 'python_web'.

.[root@docker ~]#
----
docker exec -it python_web bash
----

.[root@CONTAINER-ID ~]#
----
echo "Hello From My Container" > /usr/bin/tryme.txt

exit
----


.[root@docker ~]#
----
docker stop python_web

docker rm python_web
----

Now that you have cleaned up your containers, take a last look at your host.

.[root@docker ~]#
----
cat /usr/bin/tryme.txt
----

How was it possible that a process running in a containerized namespace was able to affect the filesystem of our host.  
  
WARNING: Containers require strong INFOSEC practices.  Merely deploying applications in native containers does NOT provide ANY additional security benefits.

This seems rather innocent, but consider the normal use of containers today:

  . A developer wants to deploy an application using mongo-db
  . The developer pulls a mongo-db container image from Docker Hub
  . The developer builds the application and integrates it with the container image
  . The developer deploys the container image
  . That was easy...

Containerized applications are easy to deploy, after all that is one of the key benefits.  However, some key points need to be asked and addressed:

  . What's inside?  (ie: known vulnerabilities and/or trojans)
  . Who will maintain and patch the base image going forward? (ie: unknown vulnerabilities, fixes, updates)
  . How will YOU maintain and patch image customizations going forward? (ie: audit)
  . How can you protect your environment from weak design practices? (ie: everything does not need to run as root)

=== Manual Incremental Security Improvements

.[root@master ~]#
----
cheat-docker-webserver-ro.sh
----

[NOTE]
====
_Native command(s) to deploy SimpleHTTPServer Read-Only_
----
docker run -d --name="python_web" \
    -v /usr/bin:/usr/bin:ro \
    -v /usr/lib64:/usr/lib64:ro \
    -v /var/www/html:/var/www/html:ro \
    -w /var/www/html  \
    -p 8000:8000 \
    rhel7 /bin/python -m SimpleHTTPServer 8000
----
====


.[root@docker ~]#
----
docker exec -it python_web bash
----


.[root@CONTAINER-ID ~]#
----
echo "Hello From My Container" > /usr/bin/tryme-again.txt

exit
----

.[root@docker ~]#
----
cat /usr/bin/tryme-again.txt
----

.Your output should look like this
[source,indent=4]
----
bash: /usr/bin/tryme-again.txt: Read-only file system
----

.[root@docker ~]#
----
docker stop python_web

docker rm python_web
----



=== Strong Security with SELinux

.[root@docker ~]#
----
setenforce enforcing
----

.[root@docker ~]#
----
getenforce
----

.[root@master ~]#
----
cheat-docker-webserver.sh
----

[NOTE]
====
_Native command(s) to deploy SimpleHTTPServer_
----
docker run -d --name="python_web" \
    -v /usr/bin:/usr/bin \
    -v /usr/lib64:/usr/lib64 \
    -v /var/www/html:/var/www/html \
    -w /var/www/html  \
    -p 8000:8000 \
    rhel7 /bin/python -m SimpleHTTPServer 8000
----
====

.[root@docker ~]#
----
docker exec -it python_web bash
----

.[root@CONTAINER-ID ~]#
----
echo "Hello From My Container" > /usr/bin/tryme-again.txt
----

.Your output should look like this
[source,indent=4]
----
bash: /usr/bin/tryme-again.txt: Permission denied
----

.[root@CONTAINER-ID ~]#
----
exit
----

.[root@docker ~]#
----
ps -efZ | grep SimpleHTTP
----

.[root@docker ~]#
----
docker stop python_web

docker rm python_web
----



== Creating a Custom Image

=== Setup

.[root@docker ~]#
----
mkdir -p /var/www/html

echo "Custom Server up and running" > /var/www/html/custom.txt

restorecon -Rv /var/www
----

=== Deployment

.[root@docker ~]#
----
docker run -d --name="python_web" \
    -v /usr/bin:/usr/bin \
    -v /usr/lib64:/usr/lib64 \
    -v /var/www/html:/mnt \
    -w /var/www/html  \
    -p 8000:8000 \
    rhel7 /bin/python -m SimpleHTTPServer 8000
----

=== Customize

.[root@docker ~]#
----
docker exec -it python_web bash
----


.[root@CONTAINER-ID ~]#
----
mkdir -p /var/www/html

cp /mnt/custom.txt /var/www/html/custom.txt

exit
----


.[root@docker ~]#
----
curl localhost:8000/custom.txt
----

=== Save

.[root@docker ~]#
----
docker stop python_web

docker commit -m "Custom Image" -a "Student" python_web custom_web
----

.[root@docker ~]#
----
docker images
----

=== Deployment of Custom Image

.[root@docker ~]#
----
docker run -d --name="custom_web" \
    -w /var/www/html  \
    -p 8000:8000 \
    custom_web /bin/python -m SimpleHTTPServer 8000
----

=== Validatationof Custom Image

.[root@docker ~]#
----
curl localhost:8000/custom.txt
----

=== Cleanup

.[root@docker ~]#
----
docker kill $(docker ps -q)

docker rm $(docker ps -a -q)
----

== Exit System

.[root@docker ~]#
----
exit
----


.[root@workstation ~]#
----
uname -n

whoami
----

.Your output should look like this
[source,indent=4]
----
workstation.example.com

root
----

Now you are ready to proceed to the next unit.

[discrete]
== End of Unit

*Next:* link:CLI-First-Time-Login.adoc[OCP CLI: First Time Login]

link:../OCP-Workshop.adoc[Return to TOC]

////
Always end files with a blank line to avoid include problems.
////
